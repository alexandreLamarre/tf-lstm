{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "mkdir: data/ptb: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir data/ptb\n",
    "!wget -q -O data/ptb/reader.py https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week3/data/ptb/reader.py\n",
    "!cp data/ptb/reader.py . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for reading words?\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset : simple-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-23 20:03:26--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz’\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M  2.02MB/s    in 32s     \n",
      "\n",
      "2021-12-23 20:04:00 (1.03 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 30\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size= 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clarifications for LSTM architecture based on the arguments:\n",
    "\n",
    "Network structure:\n",
    "\n",
    "<ul>\n",
    "    <li>In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n",
    "    </li>\n",
    "    <li>The recurrence steps is 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.</li>   \n",
    "    <li>the structure is like:\n",
    "        <ul>\n",
    "            <li>200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Input layer:\n",
    "\n",
    "<ul>\n",
    "    <li>The network has 200 input units.</li>\n",
    "    <li>Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n",
    "    </li>\n",
    "    <li>The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Hidden layer:\n",
    "\n",
    "<ul>\n",
    "    <li>Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output.</li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is adapted from the <a href=\"https://github.com/tensorflow/models?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\">PTBModel</a> example bundled with the TensorFlow source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to be done and a ton of information to process at the same time, so go over this code slowly. It may seem complex at first, but if you try to apply what you just learned about language modelling to the code you see, you should be able to understand it.\n",
    "\n",
    "This code is adapted from the <a href=\"https://github.com/tensorflow/models?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\">PTBModel</a> example bundled with the TensorFlow source code.\n",
    "\n",
    "<h3>Training data</h3>\n",
    "The story starts from data:\n",
    "<ul>\n",
    "    <li>Train data is a list of words, of size 929589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]</li>\n",
    "    <li>We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take $$floor(\\frac{N}{b \\times h})+1=1548$$ iterations for the learner to go through all sentences once. Where N is the size of the list of words, b is batch size, and h is size of each sentence. So, the number of iterators is 1548\n",
    "    </li>\n",
    "    <li>Each batch data is read from train dataset of size 600, and shape of [30x20]</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "_input_data = first_touple[0]\n",
    "_targets = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605],\n",
       "       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n",
       "          12,    3,   21,    2, 2915,    2,   12,    3,   21]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim']\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word(_input_data[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Embeddings</h3>\n",
    "We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values. However, One-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient. So, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM. \n",
    "\n",
    "The embedded vectors also get updated during the training process of the deep neural network.\n",
    "We create the embeddings for our input data. <b>embedding_vocab</b> is matrix of \\[10000x200] for all 10000 unique words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>\n",
    "It creates a \\[30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embeding_vector_size,batch_input_shape=(batch_size, num_steps),trainable=True,name=\"embedding_vocab\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 20:14:34.943044: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 200), dtype=float32, numpy=\n",
       "array([[[ 7.06777722e-03, -7.13717192e-04, -4.03195508e-02, ...,\n",
       "         -4.30422425e-02, -1.90721285e-02,  1.57894380e-02],\n",
       "        [-4.52170745e-02,  3.40266563e-02,  4.54466604e-02, ...,\n",
       "          8.55494291e-04, -2.82682907e-02, -3.89795899e-02],\n",
       "        [ 4.83803414e-02,  4.23095338e-02, -4.98698838e-02, ...,\n",
       "          2.36995332e-02,  1.55330412e-02,  4.50954698e-02],\n",
       "        ...,\n",
       "        [ 3.23509239e-02, -2.61475444e-02,  4.22129296e-02, ...,\n",
       "          2.49595568e-03,  1.02058649e-02, -4.56819795e-02],\n",
       "        [ 4.77687158e-02,  4.07776721e-02,  4.59892787e-02, ...,\n",
       "         -3.85462865e-02,  2.79747285e-02,  2.49114297e-02],\n",
       "        [-1.95396785e-02,  1.94427855e-02,  4.53920998e-02, ...,\n",
       "          1.79917850e-02, -2.56440174e-02, -2.60368120e-02]],\n",
       "\n",
       "       [[ 1.45419501e-02, -2.87338383e-02,  4.20765951e-03, ...,\n",
       "          3.65480036e-03,  3.97755392e-02, -2.78535131e-02],\n",
       "        [-1.77666433e-02, -2.59716753e-02,  4.33113836e-02, ...,\n",
       "         -2.10846588e-03, -2.18452141e-03, -2.96767484e-02],\n",
       "        [-5.48496097e-03,  7.85832480e-03,  1.66028738e-03, ...,\n",
       "         -4.68617938e-02,  1.32044666e-02,  4.98849414e-02],\n",
       "        ...,\n",
       "        [-1.53644793e-02, -3.12752612e-02,  1.45643689e-02, ...,\n",
       "          4.56137545e-02, -3.02777886e-02,  4.92680706e-02],\n",
       "        [ 3.80522944e-02, -3.07175517e-02, -2.51980554e-02, ...,\n",
       "          3.80218029e-04,  4.05270495e-02,  2.04034708e-02],\n",
       "        [ 4.08995412e-02, -1.33541115e-02,  2.40406506e-02, ...,\n",
       "          7.44016096e-03,  1.84041969e-02, -3.32929641e-02]],\n",
       "\n",
       "       [[-3.41126099e-02, -1.91397071e-02,  3.80551554e-02, ...,\n",
       "         -9.36869532e-03, -3.59825864e-02,  4.43814136e-02],\n",
       "        [-1.09581240e-02,  3.49587910e-02, -2.39343774e-02, ...,\n",
       "         -3.62060778e-02, -4.15283330e-02,  1.37253441e-02],\n",
       "        [ 2.84392126e-02,  1.18837468e-02, -9.58275050e-04, ...,\n",
       "         -1.42767318e-02,  4.84823622e-02, -3.41102108e-02],\n",
       "        ...,\n",
       "        [-1.26640573e-02,  1.39687918e-02, -3.95127423e-02, ...,\n",
       "         -3.70364264e-03,  1.36345625e-03,  4.27187197e-02],\n",
       "        [ 2.19190232e-02, -4.41916473e-02, -1.20181665e-02, ...,\n",
       "         -1.01042017e-02, -4.42973860e-02, -2.40078326e-02],\n",
       "        [-2.63884310e-02,  3.90195735e-02, -2.46784929e-02, ...,\n",
       "          4.10217308e-02, -1.56548508e-02, -2.22288612e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.55537163e-02, -3.21067795e-02,  2.51976736e-02, ...,\n",
       "         -2.06346996e-02, -2.36042626e-02,  3.17958109e-02],\n",
       "        [ 3.73547189e-02, -4.48552258e-02, -2.62847543e-02, ...,\n",
       "          2.76285745e-02, -4.46856022e-04, -4.53233235e-02],\n",
       "        [-1.43830180e-02,  9.58670303e-03, -3.75866070e-02, ...,\n",
       "         -2.31100451e-02,  4.85529639e-02, -3.32488641e-02],\n",
       "        ...,\n",
       "        [-3.51495631e-02, -2.26173755e-02,  3.66953276e-02, ...,\n",
       "         -2.08435897e-02,  3.17761190e-02, -2.67833360e-02],\n",
       "        [-2.08385475e-02, -8.72898102e-03, -4.44501042e-02, ...,\n",
       "         -1.21382251e-02, -3.54510322e-02,  4.98646535e-02],\n",
       "        [ 2.42020153e-02,  2.00377591e-02, -3.86754870e-02, ...,\n",
       "         -4.73080166e-02,  2.82612555e-02,  2.91115679e-02]],\n",
       "\n",
       "       [[ 8.74329358e-04,  4.36410047e-02,  2.15916149e-02, ...,\n",
       "          4.41501290e-03, -2.35346686e-02,  9.00850445e-03],\n",
       "        [-2.20685247e-02,  1.82597898e-02,  4.92287613e-02, ...,\n",
       "          3.20782773e-02, -2.63347477e-03, -6.88136742e-03],\n",
       "        [ 2.84392126e-02,  1.18837468e-02, -9.58275050e-04, ...,\n",
       "         -1.42767318e-02,  4.84823622e-02, -3.41102108e-02],\n",
       "        ...,\n",
       "        [ 1.67705528e-02,  3.46428156e-03, -9.96832922e-03, ...,\n",
       "         -4.45619226e-05, -1.38880610e-02, -1.44712813e-02],\n",
       "        [-1.77666433e-02, -2.59716753e-02,  4.33113836e-02, ...,\n",
       "         -2.10846588e-03, -2.18452141e-03, -2.96767484e-02],\n",
       "        [-1.04854219e-02,  1.31547712e-02,  2.72387005e-02, ...,\n",
       "         -2.07018610e-02, -2.79946811e-02,  1.59311555e-02]],\n",
       "\n",
       "       [[-4.89356779e-02, -4.67712767e-02,  5.04269451e-03, ...,\n",
       "          4.06826027e-02,  1.70150734e-02,  6.78629801e-03],\n",
       "        [ 3.61842550e-02,  1.87781192e-02,  4.54060324e-02, ...,\n",
       "          1.74940713e-02, -6.14865869e-03, -5.18921763e-03],\n",
       "        [ 2.14316733e-02, -2.72257328e-02, -4.74875346e-02, ...,\n",
       "         -1.82293057e-02,  4.52152602e-02,  4.11520489e-02],\n",
       "        ...,\n",
       "        [ 3.41456011e-03,  1.46825649e-02, -9.28318501e-03, ...,\n",
       "          9.45670530e-03, -3.65776941e-03, -9.18934494e-03],\n",
       "        [-2.65927911e-02, -4.03127782e-02, -4.25838605e-02, ...,\n",
       "         -4.35086489e-02, -1.05353370e-02,  3.95005681e-02],\n",
       "        [ 3.27687599e-02,  4.33013700e-02,  2.91191377e-02, ...,\n",
       "          2.60444731e-03,  3.06162722e-02, -3.76335867e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = embedding_layer(_input_data)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\n",
    "stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.inital_state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(30, 200) dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.inital_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 128), dtype=float32, numpy=\n",
       "array([[[ 4.6643944e-04, -2.4818932e-04, -1.3688179e-03, ...,\n",
       "         -2.8810048e-04,  2.0363746e-04, -1.5789787e-03],\n",
       "        [ 3.4833356e-04, -1.3167352e-03, -2.2515478e-04, ...,\n",
       "         -6.8017666e-04,  5.0922175e-04, -1.9137327e-03],\n",
       "        [-3.9458586e-04, -1.6466526e-03,  6.1071373e-04, ...,\n",
       "         -4.7081194e-04,  2.7425642e-04, -1.8721533e-03],\n",
       "        ...,\n",
       "        [-4.1399417e-03, -2.1895585e-03, -1.6880886e-03, ...,\n",
       "         -2.0617503e-03,  3.6157628e-03, -1.2149689e-03],\n",
       "        [-4.7810026e-03, -1.2516857e-03, -2.2502760e-03, ...,\n",
       "         -9.2530902e-04,  6.1835730e-03, -1.8138114e-03],\n",
       "        [-5.8719902e-03, -8.9965208e-04, -4.9977032e-03, ...,\n",
       "         -3.2204218e-04,  6.8912189e-03, -3.2980351e-03]],\n",
       "\n",
       "       [[ 1.5504799e-03,  5.5846205e-04, -8.7822642e-04, ...,\n",
       "          1.1553761e-03,  3.0453035e-04, -5.7462230e-04],\n",
       "        [ 2.6841527e-03,  1.0383232e-03, -5.9259892e-04, ...,\n",
       "          1.1543841e-03,  1.6260081e-03, -9.1079163e-04],\n",
       "        [ 2.9643746e-03,  3.0168542e-03, -7.0655974e-06, ...,\n",
       "          8.6276251e-04,  1.4098739e-03, -1.0651099e-03],\n",
       "        ...,\n",
       "        [ 4.1809869e-03,  2.9681667e-03,  2.5350151e-03, ...,\n",
       "         -7.0708361e-03,  1.2083718e-03, -2.1047348e-03],\n",
       "        [ 4.2269700e-03,  3.0758216e-03,  3.5484128e-03, ...,\n",
       "         -4.3330840e-03, -3.3750743e-04, -2.6473887e-03],\n",
       "        [ 3.7542682e-03,  3.7511736e-03,  3.4460768e-03, ...,\n",
       "         -1.5643549e-03, -2.3227467e-03, -1.0681780e-03]],\n",
       "\n",
       "       [[ 7.9717656e-04,  1.0736720e-03, -8.5249147e-04, ...,\n",
       "         -6.3891889e-04,  6.7269237e-04,  1.2519241e-03],\n",
       "        [-2.7239477e-04,  2.7011470e-03, -2.0731466e-03, ...,\n",
       "         -1.8119501e-03,  1.7454915e-03,  2.0469232e-03],\n",
       "        [-5.8675930e-04,  3.8761706e-03, -2.3389861e-03, ...,\n",
       "         -2.2645311e-03,  1.6231012e-03,  2.9482937e-03],\n",
       "        ...,\n",
       "        [-3.7700834e-03, -2.1522660e-03,  2.7380311e-03, ...,\n",
       "         -4.6497243e-03,  1.0949443e-04, -9.8328956e-04],\n",
       "        [-3.1384716e-03, -2.3520340e-03,  4.9335840e-03, ...,\n",
       "         -5.0976039e-03,  2.2938813e-03, -4.6136978e-05],\n",
       "        [-1.6235135e-03, -3.2381578e-03,  6.8515628e-03, ...,\n",
       "         -4.8540314e-03,  3.1173653e-03,  3.8825162e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.7814462e-04, -3.8491695e-03, -1.0749332e-03, ...,\n",
       "          5.0635322e-04,  7.5508823e-04, -2.5330222e-04],\n",
       "        [-1.7180925e-03, -5.8516441e-03, -1.7346672e-03, ...,\n",
       "          9.5154624e-04, -5.1136664e-04, -6.5558509e-04],\n",
       "        [-2.2840754e-03, -7.4746534e-03, -2.5117868e-03, ...,\n",
       "         -3.1063354e-04,  8.0679441e-05, -2.2028969e-03],\n",
       "        ...,\n",
       "        [-1.0916360e-03, -4.5602187e-03,  7.3402175e-03, ...,\n",
       "          3.7821424e-03,  5.5348482e-03, -2.1054284e-04],\n",
       "        [-1.9679563e-03, -5.0145872e-03,  7.4628578e-03, ...,\n",
       "          3.7503629e-03,  4.6004117e-03,  3.4826685e-04],\n",
       "        [-1.8402940e-03, -2.9561713e-03,  7.4387696e-03, ...,\n",
       "          2.9142241e-03,  3.8018483e-03,  1.7210214e-03]],\n",
       "\n",
       "       [[-9.6700154e-04, -1.6169281e-03, -5.6845829e-04, ...,\n",
       "         -6.0476805e-04, -2.3321761e-04, -4.2445658e-04],\n",
       "        [-1.1258315e-03, -2.7138656e-03, -6.5666880e-04, ...,\n",
       "          1.1931341e-03,  6.2647113e-04, -3.0540224e-04],\n",
       "        [-1.2332753e-03, -2.1100652e-03, -6.6711381e-04, ...,\n",
       "          1.6433959e-03,  1.1040099e-03,  2.3057174e-04],\n",
       "        ...,\n",
       "        [ 2.0302616e-03,  4.1864216e-03,  6.8733300e-04, ...,\n",
       "         -2.6104245e-03,  2.3632003e-03, -5.3442577e-03],\n",
       "        [ 1.5627147e-03,  5.0393045e-03,  1.4082661e-03, ...,\n",
       "         -3.0479508e-03,  3.2684214e-03, -5.1480187e-03],\n",
       "        [ 2.8145881e-04,  6.0164188e-03,  1.1300712e-03, ...,\n",
       "         -1.9466686e-03,  3.8095394e-03, -3.5168321e-03]],\n",
       "\n",
       "       [[-6.5749984e-05,  1.7026022e-04,  2.8275142e-03, ...,\n",
       "          1.6937747e-03, -7.6198700e-04,  1.9846708e-03],\n",
       "        [-2.6921614e-04,  1.7660591e-04,  4.8288954e-03, ...,\n",
       "          4.7974875e-03, -1.6166833e-03,  3.5958132e-03],\n",
       "        [-3.6849899e-04,  1.8563291e-05,  5.6890985e-03, ...,\n",
       "          5.3555435e-03, -1.8822197e-03,  5.3812237e-03],\n",
       "        ...,\n",
       "        [ 7.2319014e-04, -5.6692814e-03,  4.6214834e-03, ...,\n",
       "         -2.0334683e-03,  9.6748688e-04, -1.3151632e-03],\n",
       "        [ 1.8891431e-03, -6.2448387e-03,  5.4006879e-03, ...,\n",
       "         -1.2341751e-03,  3.1606518e-04, -6.1765197e-04],\n",
       "        [ 3.1713978e-03, -6.1392784e-03,  6.2463856e-03, ...,\n",
       "         -5.2320142e-04,  6.1147037e-04, -5.5213785e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from dense layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "dense = tf.keras.layers.Dense(vocab_size)\n",
    "logits_outputs  = dense(outputs)\n",
    "print(\"shape of the output from dense layer: \", logits_outputs.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from the activation layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "activation = tf.keras.layers.Activation('softmax')\n",
    "output_words_prob = activation(logits_outputs)\n",
    "print(\"shape of the output from the activation layer: \", output_words_prob.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of observing words in t=0 to t=20 tf.Tensor(\n",
      "[[1.00004771e-04 1.00013371e-04 1.00005236e-04 ... 1.00019432e-04\n",
      "  9.99947806e-05 1.00015393e-04]\n",
      " [1.00007434e-04 1.00018282e-04 1.00026191e-04 ... 1.00007266e-04\n",
      "  1.00005134e-04 1.00009813e-04]\n",
      " [1.00018922e-04 1.00028963e-04 1.00022211e-04 ... 1.00012512e-04\n",
      "  1.00006742e-04 1.00016674e-04]\n",
      " ...\n",
      " [1.00017482e-04 1.00005134e-04 9.99333279e-05 ... 1.00077770e-04\n",
      "  9.99056501e-05 1.00035068e-04]\n",
      " [9.99740369e-05 1.00028934e-04 9.99293843e-05 ... 1.00076257e-04\n",
      "  9.99115509e-05 9.99982876e-05]\n",
      " [9.99705080e-05 1.00038364e-04 9.99190524e-05 ... 1.00075529e-04\n",
      "  9.99311451e-05 9.99867480e-05]], shape=(20, 10000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0,0:num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6516, 6516, 5824, 5065,  245,  245,  245, 4139, 4139, 4139, 4139,\n",
       "        808, 9104, 9104, 8516, 8516, 8516, 2031, 5675, 5675])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0,0:num_steps], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss  = crossentropy(_targets, output_words_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([9.21027 , 9.210213, 9.210476, 9.210785, 9.211563, 9.210618,\n",
       "       9.210756, 9.210339, 9.210264, 9.210093], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=184.20724>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss / batch_size)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrelamarre/Desktop/fiddle/cnn/env/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "optimizer = tf.keras.optimizers.SGD(lr=lr, clipnorm=max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_vocab (Embedding)  (30, 20, 200)            2000000   \n",
      "                                                                 \n",
      " rnn (RNN)                   (30, 20, 128)             671088    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (30, 20, 10000)           1290000   \n",
      "                                                                 \n",
      " activation (Activation)     (30, 20, 10000)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layer)\n",
    "model.add(dense)\n",
    "model.add(activation)\n",
    "model.compile(loss=crossentropy, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab/embeddings:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/bias:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/bias:0',\n",
       " 'dense_1/kernel:0',\n",
       " 'dense_1/bias:0']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = 1.0\n",
    "\n",
    "\n",
    "        ###############################################################################\n",
    "        # Initializing the model using keras Sequential API  #\n",
    "        ###############################################################################\n",
    "        \n",
    "        self._model = tf.keras.models.Sequential()\n",
    "        \n",
    "        ####################################################################\n",
    "        # Creating the word embeddings layer and adding it to the sequence #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            self._embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embeding_vector_size,batch_input_shape=(self.batch_size, self.num_steps),trainable=True,name=\"embedding_vocab\")  #[10000x200]\n",
    "            self._model.add(self._embedding_layer)\n",
    "            \n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM Cells. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument  of LSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A). \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "        lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\n",
    "        # By taking in the LSTM cells as parameters, the StackedRNNCells function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of stacked simple cells.\n",
    "        stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ########################################################################################################\n",
    "        # Instantiating our RNN model and setting stateful to True to feed forward the state to the next layer #\n",
    "        ########################################################################################################\n",
    "        \n",
    "        self._RNNlayer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)\n",
    "        \n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\n",
    "        self._RNNlayer.inital_state = self._initial_state\n",
    "    \n",
    "        ############################################\n",
    "        # Adding RNN layer to keras sequential API #\n",
    "        ############################################        \n",
    "        self._model.add(self._RNNlayer)\n",
    "        \n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l1,return_sequences=True,stateful=True))\n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l2,return_sequences=True))\n",
    "        \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Instantiating a Dense layer that connects the output to the vocab_size  and adding layer to model#\n",
    "        ####################################################################################################\n",
    "        self._dense = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self._model.add(self._dense)\n",
    " \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Adding softmax activation layer and deriving probability to each class and adding layer to model #\n",
    "        ####################################################################################################\n",
    "        self._activation = tf.keras.layers.Activation('softmax')\n",
    "        self._model.add(self._activation)\n",
    "\n",
    "        ##########################################################\n",
    "        # Instantiating the stochastic gradient decent optimizer #\n",
    "        ########################################################## \n",
    "        self._optimizer = tf.keras.optimizers.SGD(lr=self._lr, clipnorm=max_grad_norm)\n",
    "        \n",
    "        \n",
    "        ##############################################################################\n",
    "        # Compiling and summarizing the model stacked using the keras sequential API #\n",
    "        ##############################################################################\n",
    "        self._model.compile(loss=self.crossentropy, optimizer=self._optimizer)\n",
    "        self._model.summary()\n",
    "\n",
    "    def crossentropy(self,y_true, y_pred):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    def train_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = self._model.trainable_variables\n",
    "        # Define the gradient clipping threshold\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass.\n",
    "            output_words_prob = self._model(_input_data)\n",
    "            # Loss value for this batch.\n",
    "            loss  = self.crossentropy(_targets, output_words_prob)\n",
    "            # average across batch and reduce sum\n",
    "            cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "        # Get gradients of loss wrt the trainable variables.\n",
    "        grad_t_list = tape.gradient(cost, tvars)\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        train_op = self._optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return cost\n",
    "\n",
    "    def test_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Testing Operation for our Model #\n",
    "        #################################################\n",
    "        output_words_prob = self._model(_input_data)\n",
    "        loss  = self.crossentropy(_targets, output_words_prob)\n",
    "        # average across batch and reduce sum\n",
    "        cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "\n",
    "        return cost\n",
    "    @classmethod\n",
    "    def instance(cls) : \n",
    "        return PTBModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# run_one_epoch takes as parameters  the model instance, the data to be fed, training or testing mode and verbose info #\n",
    "########################################################################################################################\n",
    "def run_one_epoch(m, data,is_training=True,verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.\n",
    "    iters = 0\n",
    "    \n",
    "    m._model.reset_states()\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        #y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "        if is_training : \n",
    "            loss=  m.train_batch(x, y)\n",
    "        else :\n",
    "            loss = m.test_batch(x, y)\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += loss\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "        \n",
    "\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_vocab (Embedding)  (30, 20, 200)            2000000   \n",
      "                                                                 \n",
      " rnn_2 (RNN)                 (30, 20, 128)             671088    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (30, 20, 10000)           1290000   \n",
      "                                                                 \n",
      " activation_2 (Activation)   (30, 20, 10000)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 4727.984 speed: 2024 wps\n",
      "Itr 164 of 1549, perplexity: 1092.767 speed: 2119 wps\n",
      "Itr 318 of 1549, perplexity: 851.210 speed: 2111 wps\n",
      "Itr 472 of 1549, perplexity: 706.065 speed: 2135 wps\n",
      "Itr 626 of 1549, perplexity: 601.218 speed: 2146 wps\n",
      "Itr 780 of 1549, perplexity: 534.173 speed: 2154 wps\n",
      "Itr 934 of 1549, perplexity: 481.567 speed: 2160 wps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/61/d4ct7r4d12j58g0l3r7wfjnc0000gn/T/ipykernel_90806/34673535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d : Learning rate: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Run the loop for this epoch in the training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d : Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perplexity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/61/d4ct7r4d12j58g0l3r7wfjnc0000gn/T/ipykernel_90806/3767767214.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(m, data, is_training, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/61/d4ct7r4d12j58g0l3r7wfjnc0000gn/T/ipykernel_90806/3570549736.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, _input_data, _targets)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Get gradients of loss wrt the trainable variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mgrad_t_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Define the gradient clipping threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_t_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fiddle/cnn/env/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1082\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fiddle/cnn/env/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fiddle/cnn/env/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m def _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[0m\u001b[1;32m    130\u001b[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001b[1;32m    131\u001b[0m   \"\"\"Calls the gradient function of the op.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiates the PTBModel class\n",
    "m=PTBModel.instance()   \n",
    "K = tf.keras.backend \n",
    "for i in range(max_epoch):\n",
    "    # Define the decay for this epoch\n",
    "    lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "    dcr = learning_rate * lr_decay\n",
    "    m._lr = dcr\n",
    "    K.set_value(m._model.optimizer.learning_rate,m._lr)\n",
    "    print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, m._model.optimizer.learning_rate))\n",
    "    # Run the loop for this epoch in the training mode\n",
    "    train_perplexity = run_one_epoch(m, train_data,is_training=True,verbose=True)\n",
    "    print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "    # Run the loop for this epoch in the validation mode\n",
    "    valid_perplexity = run_one_epoch(m, valid_data,is_training=False,verbose=False)\n",
    "    print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "# Run the loop in the testing mode to see how effective was our training\n",
    "test_perplexity = run_one_epoch(m, test_data,is_training=False,verbose=False)\n",
    "print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfc37a542397463672d5b27378fe7254cbb81b9f5bf4244daa279cb50273dbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
